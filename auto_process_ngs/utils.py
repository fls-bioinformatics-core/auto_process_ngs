#!/usr/bin/env python
#
#     utils: utility classes & funcs for auto_process_ngs module
#     Copyright (C) University of Manchester 2013-2016 Peter Briggs
#
########################################################################
#
# utils.py
#
#########################################################################

__version__ = "0.0.18"

"""utils

Utility classes and functions to support auto_process_ngs module.

Ultimately these should be relocated in the main 'genomics' code
tree at some point.

Classes:

- BaseFastqAttrs
- AnalysisFastq:
- AnalysisDir:
- AnalysisProject:
- AnalysisSample:
- MetadataDict:
- AnalysisDirParameters:
- AnalysisDirMetadata:
- AnalysisProjectInfo:
- ProjectMetadataFile:
- ZipArchive:
- OutputFiles:

Functions:

- bases_mask_is_paired_end:
- split_user_host_dir:
- pretty_print_rows:
- write_script_file:
- edit_file:
- paginate:

"""

#######################################################################
# Imports
#######################################################################

import sys
import os
import fnmatch
import logging
import zipfile
import pydoc
import tempfile
import applications
import bcftbx.IlluminaData as IlluminaData
import bcftbx.TabFile as TabFile
import bcftbx.JobRunner as JobRunner
import bcftbx.Pipeline as Pipeline
import bcftbx.utils as bcf_utils
from bcftbx.qc.report import strip_ngs_extensions
from bcftbx.Md5sum import md5sum
from qc.illumina_qc import QCReporter
from qc.illumina_qc import QCSample
from qc.illumina_qc import expected_qc_outputs
from qc.illumina_qc import check_qc_outputs
from .exceptions import MissingParameterFileException

# Module specific logger
logger = logging.getLogger(__name__)

#######################################################################
# Classes
#######################################################################

class BaseFastqAttrs(object):
    """
    Base class for extracting information about a Fastq file

    Instances of this class provide the follow attributes:

    fastq:            the original fastq file name
    basename:         basename with NGS extensions stripped
    extension:        full extension e.g. '.fastq.gz'
    sample_name:      name of the sample
    sample_number:    integer (or None if no sample number)
    barcode_sequence: barcode sequence (string or None)
    lane_number:      integer (or None if no lane number)
    read_number:      integer (or None if no read number)
    set_number:       integer (or None if no set number)

    Subclasses should process the supplied Fastq name and set
    these attributes appropriately.
    """
    def __init__(self,fastq):
        # Store name
        self.fastq = fastq
        # Basename and extension
        self.basename = strip_ngs_extensions(self.fastq)
        self.extension = self.fastq[len(self.basename):]
        self.basename = os.path.basename(self.basename)
        # Values that should be derived from the name
        # (should be set by subclass)
        self.sample_name = None
        self.sample_number = None
        self.barcode_sequence = None
        self.lane_number = None
        self.read_number = None
        self.set_number = None
    def __repr__(self):
        return self.basename

class AnalysisFastq(BaseFastqAttrs):
    """Class for extracting information about Fastq files

    Given the name of a Fastq file, extract data about the sample name,
    barcode sequence, lane number, read number and set number.

    The name format can be a 'full' Fastq name as generated by CASAVA or
    bcl2fastq 1.8, which follows the general form:

    <sample_name>_<barcode_sequence>_L<lane_number>_R<read_number>_<set_number>.fastq.gz

    e.g. for

    NA10831_ATCACG_L002_R1_001.fastq.gz

    sample_name = 'NA10831_ATCACG_L002_R1_001'
    barcode_sequence = 'ATCACG'
    lane_number = 2
    read_number = 1
    set_number = 1

    Alternatively it can be a full Fastq name as generated by bcl2fastq2,
    of the general form:

    <sample_name>_S<sample_number>_L<lane_number>_R<read_number>_001.fastq.gz

    e.g. for

    ES_exp1_S4_L003_R2_001.fastq.gz

    sample_name = 'ES_exp1'
    sample_number = 4
    lane_number = 3
    read_number = 2
    set_number = 1

    Alternatively it can be a 'reduced' version where one or more
    of the components has been omitted (typically because they are
    redundant in uniquely distinguishing a Fastq file within a
    set of Fastqs).

    The reduced formats are:

    <sample_name>
    <sample_name>_L<lane_number>
    <sample_name>_<barcode_sequence>
    <sample_name>_<barcode_sequence>_L<lane_number>

    with an optional suffix '_R<read_number>' for paired end sets.

    e.g.

    NA10831
    NA10831_L002
    NA10831_ATCACG
    NA10831_ATCACG_L002

    Provides the follow attributes:

    fastq:            the original fastq file name
    sample_name:      name of the sample (leading part of the name)
    sample_number:    integer (or None if no sample number)
    barcode_sequence: barcode sequence (string or None)
    lane_number:      integer (or None if no lane number)
    read_number:      integer (or None if no read number)
    set_number:       integer (or None if no set number)

    """

    def __init__(self,fastq):
        """Create and populate a new AnalysisFastq object

        Arguments:
          fastq: name of the fastq.gz (optionally can include leading path)

        """
        BaseFastqAttrs.__init__(self,fastq)
        # Base name for sample (no leading path or extension)
        fastq_base = self.basename
        try:
            i = fastq_base.index('.')
            fastq_base = fastq_base[:i]
        except ValueError:
            pass
        # Identify which part of the name is which
        #
        # Full Illumina-style names are e.g.
        # NH1_ChIP-seq_Gli1_ACAGTG_L001_R1_001
        # or
        # NH1_ChIP-seq_Gli1_S4_L003_R2_001
        #
        # We have shorter name formats where redundant parts are
        # omitted, the patterns are:
        # NAME          e.g. NH1_ChIP-seq_Gli1
        # NAME+LANE     e.g. NH1_ChIP-seq_Gli1_L001
        # NAME+TAG      e.g. NH1_ChIP-seq_Gli1_ACAGTG
        # NAME+TAG+LANE e.g. NH1_ChIP-seq_Gli1_ACAGTG_L001
        #
        # Also read number (i.e. R1 or R2) is appended but only for
        # paired end samples
        #
        # The set number is never included, except for full names
        fields = fastq_base.split('_')
        # Deal with set number first e.g. 001
        field = fields[-1]
        ##logger.debug("Test for set number %s" % field)
        if len(field) == 3 and field.isdigit():
            self.set_number = int(field)
            fields = fields[:-1]
        # Deal with trailing read number e.g. R1
        field = fields[-1]
        ##logger.debug("Test for read number %s" % field)
        if len(field) == 2 and field.startswith('R'):
            self.read_number = int(field[1])
            fields = fields[:-1]
        # Deal with trailing lane number e.g. L001
        field = fields[-1]
        ##logger.debug("Test for lane number %s" % field)
        if len(field) == 4 and field.startswith('L') and field[1:].isdigit():
            self.lane_number = int(field[1:])
            fields = fields[:-1]
        # Deal with trailing index tag e.g. ATTGCT or ATTGCT-CCTAAG
        field = fields[-1]
        ##logger.debug("Test for barcode sequence %s" % field)
        if len(fields) > 1:
            # This mustn't be the last field: if it is then it's
            # not the tag - it's the name
            is_tag = True
            for f in field.split('-'):
                for c in f:
                    is_tag = is_tag and c in 'ACGTN'
            if is_tag:
                self.barcode_sequence = field
                fields = fields[:-1]
                ##logger.debug("Identified barcode sequence as %s" % self.barcode_sequence)
            else:
                # Alternatively might be the sample number
                if field.startswith('S'):
                    try:
                        if field[1:].isdigit():
                            self.sample_number = int(field[1:])
                            fields = fields[:-1]
                    except IndexError:
                        pass
        # What's left is the name
        ##logger.debug("Remaining fields: %s" % fields)
        self.sample_name = '_'.join(fields)
        assert(self.sample_name != '')

    def __repr__(self):
        """Implement __repr__ built-in

        """
        # Reconstruct name
        fq = "%s" % self.sample_name
        if self.sample_number is not None:
            fq = "%s_S%d" % (fq,self.sample_number)
        if self.barcode_sequence is not None:
            fq = "%s_%s" % (fq,self.barcode_sequence)
        if self.lane_number is not None:
            fq = "%s_L%03d" % (fq,self.lane_number)
        if self.read_number is not None:
            fq = "%s_R%d" % (fq,self.read_number)
        if self.set_number is not None:
            fq = "%s_%03d" % (fq,self.set_number)
        return fq

class AnalysisDir:
    """Class describing an analysis directory

    Conceptually an analysis directory maps onto a sequencing run.
    It consists of one or more sets of samples from that run,
    which are represented by subdirectories.

    It is also possible to have one or more subdirectories containing
    outputs from the CASAVA or bclToFastq processing software.

    """
    def __init__(self,analysis_dir):
        """Create a new AnalysisDir instance for a specified directory

        Arguments:
          analysis_dir: name (and path) to analysis directory

        """
        # Store location
        self._analysis_dir = os.path.abspath(analysis_dir)
        self._name = os.path.basename(analysis_dir)
        self._bcl2fastq_dirs = []
        self._project_dirs = []
        self._extra_dirs = []
        self.sequencing_data = []
        self.projects = []
        self.undetermined = None
        # Metadata
        self.metadata = AnalysisDirMetadata()
        try:
            metadata_file = os.path.join(self._analysis_dir,
                                         "metadata.info")
            self.metadata.load(metadata_file)
        except Exception as ex:
            logger.warning("Failed to load metadata file %s: %s" %
                           (metadata_file,ex))
            logger.warning("Attempting to load parameter file")
            try:
                params = AnalysisDirParameters()
                parameter_file = os.path.join(self._analysis_dir,
                                         "auto_process.info")
                params.load(parameter_file,strict=False)
                # Attempt to acquire values from parameters
                for param in ('platform','run_number','source','assay'):
                    if param not in params:
                        print "-- %s: missing" % param
                        continue
                    print "-- %s: setting to '%s'" % (param,
                                                      params[param])
                    self.metadata[param] = params[param]
            except Exception as ex:
                # No parameter file either
                logger.warning("Failed to load parameters: %s" % ex)
                logger.warning("Perhaps this is not an auto_process project?")
                raise ex
        # Projects metadata
        try:
            self.projects_metadata = ProjectMetadataFile(
                os.path.join(self._analysis_dir,"projects.info"))
        except Exception as ex:
            logger.warning("Failed to load projects metadata: %s" % ex)
            self.projects_metadata = None
        # Run name
        try:
            self.run_name = self.metadata.run
        except AttributeError:
            self.run_name = self._analysis_dir[0:-len('_analysis')]
        self.run_name = os.path.basename(self.run_name)
        self.date_stamp,\
            self.instrument_name,\
            self.instrument_run_number = IlluminaData.split_run_name(
                self.run_name)
        # Look for outputs from bclToFastq and analysis projects
        logger.debug("Examining subdirectories of %s" % self._analysis_dir)
        for dirn in bcf_utils.list_dirs(self._analysis_dir):
            # Look for sequencing data
            try:
                data = IlluminaData.IlluminaData(self._analysis_dir,
                                                 unaligned_dir=dirn)
                logger.debug("- %s: sequencing data" % dirn)
                self._bcl2fastq_dirs.append(dirn)
                self.sequencing_data.append(data)
                continue
            except IlluminaData.IlluminaDataError, ex:
                pass
            # Look for analysis data
            data = AnalysisProject(dirn,os.path.join(self._analysis_dir,dirn))
            if data.is_analysis_dir:
                if dirn == 'undetermined':
                    logger.debug("- %s: undetermined indexes" % dirn)
                    self.undetermined = data
                else:
                    # Check against projects.info, if possible
                    try:
                        if not self.projects_metadata.lookup('Project',dirn):
                            logger.debug("- %s: not in projects.info" % dirn)
                            self._extra_dirs.append(dirn)
                            continue
                    except AttributeError:
                        pass
                    logger.debug("- %s: project directory" % dirn)
                    self._project_dirs.append(dirn)
                    self.projects.append(data)
                continue
            else:
                # Unidentified contents
                self._extra_dirs.append(dirn)
                logger.debug("- %s: unknown" % dirn)

    @property
    def n_projects(self):
        """Return number of projects found

        """
        return len(self.projects)

    @property
    def n_sequencing_data(self):
        """Return number of sequencing data dirs found

        """
        return len(self.sequencing_data)

    @property
    def paired_end(self):
        """Return True if run is paired end, False if single end

        """
        return reduce(lambda x,y: x and y.info.paired_end,self.projects,True)

    def get_projects(self,pattern=None,include_undetermined=True):
        """Return the analysis projects in a list

        By default returns all projects within the analysis
        
        If the 'pattern' is not None then it should be a simple pattern
        used to match against available names to select a subset of
        projects (see bcf_utils.name_matches).

        If 'include_undetermined' is True then the undetermined
        project will also be included; otherwise it will be omitted.

        """
        projects = [p for p in self.projects]
        if include_undetermined and self.undetermined:
            projects.append(self.undetermined)
        # Filter on pattern
        if pattern is not None:
            projects = filter(lambda p: fnmatch.fnmatch(p.name,pattern),
                              projects)
        return projects
        
class AnalysisProject:
    """Class describing an analysis project

    Conceptually an analysis project consists of a set of samples
    from a single sequencing experiment, plus associated data e.g.
    QC results.

    Practically an analysis project is represented by a directory
    with a set of fastq files.

    Provides the following properties:

    name        : name of the project
    dirn        : associated directory (full path)
    fastq_dirs  : list of all subdirectories with fastq files (relative
                  to dirn)
    fastq_dir   : directory with 'primary' fastq file set (full path)
    fastqs      : list of fastq files in fastq_dir
    samples     : list of AnalysisSample objects generated from fastq_dir
    multiple_fastqs: True if at least one sample has more than one fastq
                     file per read associated with it
    fastq_format: either 'fastqgz' or 'fastq'

    There is also an 'info' property with the following additional
    properties:

    run         : run name
    user        : user name
    PI          : PI name
    library_type: library type, either None or e.g. 'RNA-seq' etc
    organism    : organism, either None or e.g. 'Human' etc
    platform    : sequencing platform, either None or e.g. 'miseq' etc
    comments    : additional comments, either None or else string of text
    paired_end  : True if data is paired end, False if not

    It is possible for a project to have multiple sets of associated
    fastq files, held within separate subdirectories of the project
    directory. A list of subdirectory names with fastq sets can be
    accessed via the 'fastq_dirs' property.

    By default the 'primary' set will be in the 'fastqs' subdirectory.
    This can be changed using the 'fastq_dir' argument when
    instantiating the AnalysisProject object. A different directory
    can also be specified using the 'fastq_dir' argument when creating
    the project directory.

    The fastq set can also be switched using the 'use_fastq_dir'
    method.
    """
    def __init__(self,name,dirn,user=None,PI=None,library_type=None,
                 organism=None,run=None,comments=None,platform=None,
                 fastq_attrs=None,fastq_dir=None):
        """Create a new AnalysisProject instance

        Arguments:
          name: name of the project
          dirn: project directory (can be full or relative path)
          user: optional, specify name of the user
          PI: optional, specify name of the principal investigator
          library_type: optional, specify library type e.g. 'RNA-seq',
            'miRNA' etc
          organism: optional, specify organism e.g. 'Human', 'Mouse'
            etc
          platform: optional, specify sequencing platform e.g 'miseq'
          run: optional, name of the run
          comments: optional, free text comments associated with the
            run
          fastq_attrs: optional, specify a class to use to get
            attributes from a Fastq file name (e.g. sample name, read
            number etc). The supplied class should be a subclass of
            BaseFastqAttrs; defaults to 'AnalysisFastq'.
          fastq_dir: optional, explicitly specify the subdirectory
            holding the set of Fastq files to load; defaults to
            'fastq' (if present) or to the top-level of the project
            directory (if absent).
        """
        self.name = name
        self.dirn = os.path.abspath(dirn)
        self.fastq_dir = None
        self.fastq_dirs = []
        self.fastq_format = None
        self.samples = []
        self.info = AnalysisProjectInfo()
        self.info_file = os.path.join(self.dirn,"README.info")
        # Function to use for getting Fastq information
        if fastq_attrs is None:
            self.fastq_attrs = AnalysisFastq
        else:
            self.fastq_attrs = fastq_attrs
        # Populate from the directory contents
        self.populate(fastq_dir=fastq_dir)
        # (Re)set metadata
        if run is not None:
            self.info['run'] = run
        if user is not None:
            self.info['user'] = user
        if PI is not None:
            self.info['PI'] = PI
        if library_type is not None:
            self.info['library_type'] = library_type
        if organism is not None:
            self.info['organism'] = organism
        if platform is not None:
            self.info['platform'] = platform
        if comments is not None:
            self.info['comments'] = comments

    def populate(self,fastq_dir=None):
        """Populate data structure from directory contents

        """
        if not os.path.exists(self.dirn):
            # Nothing to do, yet
            return
        # Identify possible fastq subdirectories
        fastq_dirs = []
        for d in bcf_utils.list_dirs(self.dirn):
            fq_dir = os.path.join(self.dirn,d)
            fastqs = self.find_fastqs(fq_dir)
            if fastqs:
                fastq_dirs.append(d)
        # Also check top-level dir
        if self.find_fastqs(self.dirn):
            fastq_dirs.append('.')
        self.fastq_dirs = fastq_dirs
        logger.debug("Possible fastq dirs: %s" %
                     ','.join(self.fastq_dirs))
        # Set primary fastq file directory
        if not self.fastq_dirs:
            logger.debug("No fastq dirs located for %s" % self.dirn)
            return
        if fastq_dir is None:
            if 'fastqs' in self.fastq_dirs:
                fastq_dir = 'fastqs'
            else:
                fastq_dir = self.fastq_dirs[0]
        else:
            if fastq_dir not in self.fastq_dirs:
                logger.warning("Requested fastqs dir '%s' not in list "
                               "of possible dirs %s" %
                               (fastq_dir,
                                ', '.join(self.fastq_dirs)))
        self.fastq_dir = os.path.normpath(
            os.path.join(self.dirn,fastq_dir))
        # Collect fastq files
        fastqs = self.find_fastqs(self.fastq_dir)
        if fastqs:
            self.fastq_format = self.determine_fastq_format(fastqs[0])
        logger.debug("Assigning fastqs to samples...")
        self.samples = []
        for fq in fastqs:
            name = self.fastq_attrs(fq).sample_name
            try:
                sample = self.get_sample(name)
            except KeyError:
                sample = AnalysisSample(name,
                                        fastq_attrs=self.fastq_attrs)
                self.samples.append(sample)
            sample.add_fastq(os.path.normpath(
                os.path.join(self.fastq_dir,fq)))
        logger.debug("Listing samples and files:")
        for sample in self.samples:
            logger.debug("* %s: %s" % (sample.name,sample.fastq))
        # Get data from info file, if present
        if os.path.isfile(self.info_file):
            self.info.load(self.info_file)
        # Set paired_end flag for project
        paired_end = True
        for sample in self.samples:
            paired_end = (paired_end and sample.paired_end)
        self.info['paired_end'] = paired_end

    def find_fastqs(self,dirn):
        """
        Return list of Fastq files found in directory
        """
        logger.debug("Searching '%s' for fastqs" % dirn)
        fastq_tuples = Pipeline.GetFastqGzFiles(dirn)
        if not fastq_tuples:
            logger.debug("No fastq.gz files found")
            fastq_tuples = Pipeline.GetFastqFiles(dirn)
            if not fastq_tuples:
                logger.debug("No fastq files found")
        # Unpack tuples
        # see https://stackoverflow.com/a/952952/579925
        fastqs = [item for sublist in fastq_tuples for item in sublist]
        return fastqs

    def determine_fastq_format(self,fastq):
        """
        Return type for Fastq file ('fastq' or 'fastqgz')
        """
        if fastq.endswith('.gz'):
            return 'fastqgz'
        else:
            return 'fastq'

    def use_fastq_dir(self,fastq_dir):
        """
        Switch fastq directory and repopulate
        """
        if fastq_dir not in self.fastq_dirs:
            raise Exception("Fastq dir '%s' not found in "
                            "project '%s' (%s)" %
                            (fastq_dir,self.name,self.dirn))
        self.populate(fastq_dir=fastq_dir)

    def create_directory(self,illumina_project=None,fastqs=None,
                         fastq_dir=None,
                         short_fastq_names=False,
                         link_to_fastqs=False):
        """Create and populate analysis directory for an IlluminaProject

        Creates a new directory corresponding to the AnalysisProject
        object, and optionally also populates with links to FASTQ files
        from a supplied IlluminaProject object.

        The directory structure it creates is:

        dir/
           fastqs/
           logs/
           ScriptCode/

        It also creates an info file with metadata about the project.

        Arguments:
          illumina_project: (optional) populated IlluminaProject object
            from which the analysis directory will be populated
          fastqs: (optional) list of fastq files to import
          fastq_dir: (optional) name of subdirectory to put fastq files
            into; defaults to 'fastqs'
          short_fastq_names: (optional) if True then transform fastq file
            names to be the shortest possible unique names; if False
            (default) then use the original fastq names
          link_to_fastqs: (optional) if True then make symbolic links to
            to the fastq files; if False (default) then make hard links
    
        """
        logger.debug("Creating analysis directory for project '%s'" % self.name)
        # Check for & create directory
        if os.path.exists(self.dirn):
            logger.warning("Directory %s already exists" % self.dirn)
        else:
            logger.debug("Making analysis directory %s" % self.dirn)
            bcf_utils.mkdir(self.dirn,mode=0775)
        # Make a 'ScriptCode' directory
        scriptcode_dir = os.path.join(self.dirn,"ScriptCode")
        bcf_utils.mkdir(scriptcode_dir,mode=0775)
        # Put a file in ScriptCode to make sure it's
        # not pruned on subsequent rsync operations
        fp = open(os.path.join(self.dirn,'ScriptCode','README.txt'),'w')
        fp.write("The ScriptCode directory is a place to put custom scripts and programs")
        fp.close()
        # Make a 'fastqs' directory
        if fastq_dir is None:
            fastq_dir = "fastqs"
        fastq_dir = os.path.join(self.dirn,fastq_dir)
        bcf_utils.mkdir(fastq_dir,mode=0775)
        # Check for & create links to fastq files
        if fastqs is None:
            # Make a list of fastqs to import from the supplied
            # IlluminaProject object
            fastqs = []
            if illumina_project is not None:
                for sample in illumina_project.samples:
                    for fastq in sample.fastq:
                        fastqs.append(os.path.join(sample.dirn,fastq))
        if short_fastq_names:
            # Get mapping to (shortened) unique names
            fastq_names = IlluminaData.get_unique_fastq_names(fastqs)
        else:
            # Use full names
            fastq_names = {}
            for fq in fastqs:
                fastq_names[fq] = os.path.basename(fq)
        for fastq in fastqs:
            target_fq = os.path.join(fastq_dir,fastq_names[fastq])
            if os.path.exists(target_fq):
                logger.warning("Target '%s' already exists" % target_fq)
            else:
                if link_to_fastqs:
                    logger.debug("Making symlink to %s" % fastq)
                    bcf_utils.mklink(fastq,target_fq,relative=True)
                else:
                    logger.debug("Making hard link to %s" % fastq)
                    os.link(fastq,target_fq)
        # Populate
        self.populate(fastq_dir=os.path.basename(fastq_dir))
        # Update metadata information summarising the samples
        n_samples = len(self.samples)
        if n_samples == 0:
            sample_description = "No samples"
        else:
            sample_description = "%s %s" % (n_samples,
                                            'sample' if n_samples == 1 else 'samples')
            sample_description += " (%s" % \
                                  bcf_utils.pretty_print_names(
                                      [s.name for s in self.samples])
            if self.multiple_fastqs:
                sample_description += ", multiple fastqs per sample"
            sample_description += ")"
        self.info['samples'] = sample_description
        # Save metadata
        self.info.save(self.info_file)

    @property
    def exists(self):
        """Check if analysis project directory already exists

        """
        return os.path.exists(self.dirn)

    @property
    def is_analysis_dir(self):
        """Determine if directory really is an analysis project

        """
        return len(self.samples) > 0

    @property
    def qc_dir(self):
        # Return path to qc dir, if present
        qc_dir = os.path.join(self.dirn,'qc')
        if os.path.exists(qc_dir):
            return qc_dir
        else:
            return None

    @property
    def qc(self):
        # Return IlluminaQCReporter object for this project
        if self.qc_dir is None:
            return None
        else:
            return QCReporter(self)

    def qc_report(self,force=False):
        # Generate HTML and zipped QC reports
        # Return name of zip file, or None if there is a problem
        # Set force=True to force reports to be generated
        if not (force or self.verify_qc()):
            logger.debug("Failed to generate QC report for %s: QC "
                          "not verified and force not specified"
                          % self.name)
            return None
        # Create HTML report
        logger.debug("Creating HTML QC report for %s" % self.name)
        try:
            if self.info.run is not None:
                title = "%s/%s: QC report" % (self.info.run,self.name)
            else:
                title = "%s: QC report" % self.name
            report_html = os.path.join(self.dirn,"qc_report.html")
            self.qc.report(title=title,
                           filename=report_html,
                           relative_links=True)
        except Exception as ex:
            logger.error("Exception trying to generate QC report "
                         "for %s: %s" % (self.name,ex))
            return None
        # Create zip file
        logger.debug("Creating zip archive of QC report for %s" %
                      self.name)
        try:
            analysis_dir = os.path.basename(os.path.dirname(self.dirn))
            report_zip = os.path.join(self.dirn,
                                      "qc_report.%s.%s.zip" %
                                      (self.name,analysis_dir))
            zip_file = ZipArchive(report_zip,relpath=self.dirn,
                                  prefix="qc_report.%s.%s" %
                                  (self.name,analysis_dir))
            # Add the HTML report
            zip_file.add_file(report_html)
            # Add the FastQC and screen files
            for sample in self.qc.samples:
                for fastqs in sample.fastq_pairs:
                    for fq in fastqs:
                        logger.debug("Adding QC outputs for %s" % fq)
                        for f in expected_qc_outputs(fq,self.qc_dir):
                            if f.endswith('.zip'):
                                # Exclude .zip file
                                continue
                            if os.path.exists(f):
                                zip_file.add(f)
            # Finished
            return report_zip
        except Exception as ex:
            logger.error("Exception trying to generate zip archive "
                         "of QC report for %s: %s" % (self.name,ex))
            return None

    @property
    def multiple_fastqs(self):
        # Determine if there are multiple fastqs per sample
        if not len(self.samples):
            return False
        else:
            return reduce(lambda x,y: x and y,
                          [len(s.fastq_subset(read_number=1)) > 1 for s in self.samples])

    @property
    def fastqs(self):
        """Return a list of fastqs

        """
        fastqs = []
        for s in self.samples:
            fastqs.extend(s.fastq)
        return fastqs

    @property
    def fastqs_are_symlinks(self):
        """Return True if fastq files are symbolic links, False if not

        """
        for s in self.samples:
            if s.fastqs_are_symlinks:
                return True
        return False

    def verify_qc(self):
        # Verify if the QC was successful
        try:
            return self.qc.verify()
        except AttributeError:
            return False

    def get_sample(self,name):
        """Return sample that matches 'name'

        Arguments:
          name: name of a sample

        Returns:
          AnalysisSample object with the matching name; raises
          KeyError exception if no match is found.

        """
        for sample in self.samples:
            if sample.name == name: return sample
        raise KeyError, "No matching sample for '%s'" % name

    def get_samples(self,pattern):
        """Return list of sample matching pattern

        Arguments:
          pattern: simple 'glob' style pattern

        Returns:
          Python list of samples with names matching the supplied
          pattern (or an empty list if no names match).

        """
        samples = []
        for sample in self.samples:
            if bcf_utils.name_matches(sample.name,pattern):
                samples.append(sample)
        return samples

    def prettyPrintSamples(self):
        """Return a nicely formatted string describing the sample names

        Wraps a call to 'pretty_print_names' function.

        """
        return bcf_utils.pretty_print_names(self.samples)

class AnalysisSample:
    """Class describing an analysis sample

    An analysis sample consists of a set of fastqs file corresponding
    to single sample.

    AnalysisSample has the following properties:

    name      : name of the sample
    fastq     : list of fastq files associated with the sample
    paired_end: True if sample is paired end, False if not

    """

    def __init__(self,name,fastq_attrs=None):
        """Create a new AnalysisSample instance

        Arguments:
          name: sample name
          fastq_attrs: optional, specify a class to use to get
            attributes from a Fastq file name (e.g. sample name, read
            number etc). The supplied class should be a subclass of
            BaseFastqAttrs; defaults to 'AnalysisFastq'.

        """
        self.name = name
        self.fastq = []
        self.paired_end = False
        # Function to use for getting Fastq information
        if fastq_attrs is None:
            self.fastq_attrs = AnalysisFastq
        else:
            self.fastq_attrs = fastq_attrs

    def add_fastq(self,fastq):
        """Add a reference to a fastq file in the sample

        Arguments:
          fastq: full path for the fastq file

        """
        assert(os.path.isabs(fastq))
        self.fastq.append(fastq)
        # Sort fastq's into order
        self.fastq.sort()
        # Check paired-end status
        if not self.paired_end:
            fq = self.fastq_attrs(fastq)
            if fq.read_number == 2:
                self.paired_end = True

    def fastq_subset(self,read_number=None):
        """Return a subset of fastq files from the sample

        Arguments:
          read_number: select subset based on read_number (1 or 2)

        Returns:
          List of full paths to fastq files matching the selection criteria.

        """
        # Build list of fastqs that match the selection criteria
        fastqs = []
        for fastq in self.fastq:
            fq = self.fastq_attrs(fastq)
            if fq.read_number is None:
                logger.debug("Unable to determine read number for %s, assume R1" % fastq)
                fq_read_number = 1
            else:
                fq_read_number = fq.read_number
            if fq_read_number == read_number:
                fastqs.append(fastq)
        # Sort into dictionary order and return
        fastqs.sort()
        return fastqs

    @property
    def fastqs_are_symlinks(self):
        """Return True if fastq files are symlinked, False if not

        """
        for fastq in self.fastq:
            if os.path.islink(fastq):
                return True
        return False

    def qc_sample(self):
        """Fetch QCSample object for this sample

        Returns:
          Populated QCSample object.

        """
        return QCSample(self)

    def verify_qc(self,qc_dir,fastq):
        """Check if QC completed for a fastq file

        Arguments:
          qc_dir: name of the QC directory
          fastq : fastq file to get the QC information for

        Returns:
          True if QC completed correctly, False otherwise.

        """
        present,missing = check_qc_outputs(fastq,qc_dir)
        if missing:
            return False
        return True

    def __repr__(self):
        """Implement __repr__ built-in

        Return string representation for the sample -
        i.e. the sample name.

        """
        return str(self.name)

class MetadataDict(bcf_utils.AttributeDictionary):
    """Class for storing metadata in an analysis project

    Provides storage for arbitrary data items in the form of
    key-value pairs, which can be saved to and loaded from
    an external file.

    The data items are defined on instantiation via a dictionary
    supplied to the 'attributes' argument. For example:

    Create a new metadata object:
    >>> metadata = MetadataDict(attributes={'salutation':'Salutation',
    ...                                     'valediction': 'Valediction'})
 
    The dictionary keys correspond to the keys in the MetadataDict
    object; the corresponding values are the keys that are used
    when saving and loading the data to and from a file.

    Set attributes:
    >>> metadata['salutation'] = 'hello'
    >>> metadata['valediction'] = 'goodbye'

    Retrieve values:
    >>> print "Salutation is %s" % metadata.salutation

    Save to file:
    >>> metadata.save('metadata.tsv')

    Load data from a file:
    >>> metadata = MetadataDict('metadata.tsv')
    or
    >>> metadata = MetadataDict()
    >>> metadata.load('metadata.tsv')

    List items with 'null' values:
    >>> metadata.null_items()

    The external file storage is intended to be readable by
    humans so longer names are used to describe the keys; also
    Python None values are stored as '.', and True and False
    values are stored as 'Y' and 'N' respectively. These values
    are automatically converted back to the Python equivalents
    on reload.

    """

    def __init__(self,attributes=dict(),order=None,filen=None):
        """Create a new MetadataDict object

        By default an empty metadata object is created
        i.e. all attributes will have be None.

        If an input file is specified then the attributes
        will be assigned values according to the key-value
        pairs in that file.

        Arguments:
          attributes: dictionary defining metadata items
          filen: (optional) name of the tab-delimited file
            with key-value pairs to load in.

        """
        bcf_utils.AttributeDictionary.__init__(self)
        self.__filen = filen
        # Set up empty metadata attributes
        self.__attributes = attributes
        for key in self.__attributes:
            self[key] = None
        if self.__filen:
            # Load data from external file
            load(self,self.__filen)
        # Set up order of keys for output
        if order is None:
            self.__key_order = self.__attributes.keys()
            self.__key_order.sort()
        else:
            # Use supplied key order
            self.__key_order = []
            for key in order:
                if key in self.__attributes:
                    self.__key_order.append(key)
                else:
                    raise KeyError,"Key '%s' not defined in attributes"
            # Append keys not explicitly listed in the order
            extra_keys = []
            for key in self.__attributes:
                if key not in self.__key_order:
                    extra_keys.append(key)
            if extra_keys:
                extra_keys.sort()
                self.__key_order.extend(extra_keys)

    def __setitem__(self,key,value):
        if key in self.__attributes:
            bcf_utils.AttributeDictionary.__setitem__(self,key,value)
        else:
            raise AttributeError,"Key '%s' not defined" % key

    def __iter__(self):
        return iter(self.__key_order)

    def load(self,filen,strict=True):
        """Load key-value pairs from a tab-delimited file
        
        Loads the key-value pairs from a previously created
        tab-delimited file written by the 'save' method.

        Note that this overwrites any existing values
        already assigned to keys within the metadata object.

        Arguments:
          filen (str): name of the tab-delimited file with
            key-value pairs
          strict (boolean): if True (default) then discard
            items in the input file which are missing from
            the definition; if False then add them to the
            definition.

        """
        self.__filen = filen
        metadata = TabFile.TabFile(filen)
        for line in metadata:
            try:
                # Get data from file and convert special values
                # to Python equivalents
                attr,value = line[0],line[1]
                if value == '.' or value == 'None':
                    value = None
                elif value == 'Y' or value == 'True':
                    value = True
                elif value == 'N' or value == 'False':
                    value = False
                # Locate dictionary key matching file key
                found_key = False
                for key in self.__attributes:
                    if self.__attributes[key] == attr:
                        self[key] = value
                        found_key = True
                        break
                if not found_key:
                    if strict:
                        logger.debug("Unrecognised key in %s: %s"
                                     % (filen,attr))
                    else:
                        logger.debug("Adding key from %s: %s"
                                     % (filen,attr))
                        self.__attributes[attr] = attr
                        self.__key_order.append(attr)
                        self[attr] = value
            except IndexError:
                logger.warning("Bad line in %s: %s" % (filen,line))

    def save(self,filen=None):
        """Save metadata to tab-delimited file

        Writes key-value paires to a tab-delimited file.
        The data can be recovered using the 'load' method.
 
        Note that if the specified file already exists then
        it will be overwritten.

        Arguments:
          filen: name of the tab-delimited file with key-value
            pairs; if None then the file specified when the
            object was instantiated will be used instead.

        """
        metadata = TabFile.TabFile()
        for key in self.__key_order:
            # Retrieve value and convert to appropriate
            # format for persistent storage
            value = self[key]
            if value is None:
                value = '.'
            elif value is True:
                value = 'Y'
            elif value is False:
                value = 'N'
            # Get the equivalent file key
            attr = self.__attributes[key]
            # Store in the file
            metadata.append(data=(attr,value))
        # Write the file
        if filen is not None:
            self.__filen = filen
        metadata.write(self.__filen)

    def null_items(self):
        """
        Return a list of data items with 'null' values

        """
        null_items = []
        for key in self.__key_order:
            if self[key] is None:
                null_items.append(key)
        return null_items

class AnalysisDirParameters(MetadataDict):
    """Class for storing parameters in an analysis directory

    Provides a set of data items representing parameters for
    the current analysis, which are loaded from and saved to
    an external file.

    The parameter data items are:

    analysis_dir: path to the analysis directory
    data_dir: path to the directory holding the raw sequencing data
    platform: sequencing platform e.g. 'miseq'
    sample_sheet: path to the customised SampleSheet.csv file
    bases_mask: bases mask string
    project_metadata: name of the project metadata file
    primary_data_dir: directory used to hold copies of primary data
    unaligned_dir: output directory for bcl2fastq conversion
    barcode_analysis_dir: directory holding barcode analysis outputs
    stats_file: name of file with per-fastq statistics
    per_lane_stats_file: name of file with per-lane statistics

    """
    def __init__(self,filen=None):
        """Create a new AnalysisDirParameters object

        Arguments:
          filen (str): (optional) name of the tab-delimited
            file with key-value pairs to load in.

        """
        MetadataDict.__init__(self,
                              attributes = {
                                  'analysis_dir':'analysis_dir',
                                  'data_dir':'data_dir',
                                  'sample_sheet':'sample_sheet',
                                  'bases_mask':'bases_mask',
                                  'project_metadata':'project_metadata',
                                  'primary_data_dir':'primary_data_dir',
                                  'unaligned_dir':'unaligned_dir',
                                  'barcode_analysis_dir':'barcode_analysis_dir',
                                  'stats_file':'stats_file',
                                  'per_lane_stats_file':'per_lane_stats_file',
                              },
                              filen=filen)

class AnalysisDirMetadata(MetadataDict):
    """Class for storing metadata about an analysis directory

    Provides a set of data items representing metadata about
    the current analysis, which are loaded from and saved to
    an external file.

    The metadata items are:

    run_name: name of the run
    run_number: run number assigned by local facility
    source: source of the data (e.g. local facility)
    platform: sequencing platform e.g. 'miseq'
    assay: the 'assay' from the IEM SampleSheet e.g. 'Nextera XT'
    bcl2fastq_software: info on the Bcl conversion software used

    """
    def __init__(self,filen=None):
        """Create a new AnalysisDirMetadata object

        Arguments:
          filen (str): (optional) name of the tab-delimited
            file with key-value pairs to load in.

        """
        MetadataDict.__init__(self,
                              attributes = {
                                  'run_name':'run_name',
                                  'run_number': 'run_number',
                                  'source': 'source',
                                  'platform':'platform',
                                  'assay': 'assay',
                                  'bcl2fastq_software': 'bcl2fastq_software',
                                  'instrument_name': 'instrument_name',
                                  'instrument_datestamp': 'instrument_datestamp',
                                  'instrument_run_number': 'instrument_run_number',
                              },
                              order=('run_name',
                                     'platform',
                                     'run_number',
                                     'source',),
                              filen=filen)

class AnalysisProjectInfo(MetadataDict):
    """Class for storing metadata in an analysis project

    Provides a set of metadata items which are loaded from
    and saved to an external file.

    The data items are:

    run: the name of the sequencing run
    user: the user associated with the project
    PI: the principal investigator associated with the project
    organism: the organism associated with the project
    library_type: the library type e.g. 'RNA-seq'
    platform: the platform name e.g. 'miseq'
    paired_end: True if the data is paired end, False if not
    samples: textual description of the samples in the project
    comments: free-text comments

    """
    def __init__(self,filen=None):
        """Create a new AnalysisProjectInfo object

        Arguments:
          filen: (optional) name of the tab-delimited file
            with key-value pairs to load in.

        """
        MetadataDict.__init__(self,
                              attributes = {
                                  'run':'Run',
                                  'platform':'Platform',
                                  'user':'User',
                                  'PI':'PI',
                                  'organism':'Organism',
                                  'library_type':'Library type',
                                  'paired_end':'Paired_end',
                                  'samples':'Samples',
                                  'comments':'Comments',
                              },
                              order = (
                                  'run',
                                  'platform',
                                  'user',
                                  'PI',
                                  'organism',
                                  'library_type',
                                  'paired_end',
                                  'samples',
                                  'comments',
                              ),
                              filen=filen)

class ProjectMetadataFile(TabFile.TabFile):
    """File containing metadata about multiple projects in analysis dir
 
    The file consists of a header line plus one line per project
    with the following tab-delimited fields:

    Project: name of the project
    Samples: list/description of sample names
    User: name(s) of the associated user(s)
    Library: the library type
    Organism: name(s) of the organism(s)
    PI: name(s) of the associated principal investigator(s)
    Comments: free text containing additional information
              about the project

    Any fields set to None will be written to file with a '.'
    placeholder.

    """
    def __init__(self,filen=None):
        """Create a new ProjectsMetadataFile instance

        Arguments:
          filen: (optional) name of an existing file to read
            projects in from.

        """
        # List of expected fields
        # Add new fields to this list
        self._default_fields = ('Project',
                                'Samples',
                                'User',
                                'Library',
                                'Organism',
                                'PI',
                                'Comments')
        # Map keywords to column names
        self._kwmap = { 'Project': 'project_name',
                        'Samples': 'sample_names',
                        'User': 'user',
                        'Library': 'library_type',
                        'Organism': 'organism',
                        'PI' : 'PI',
                        'Comments': 'comments', }
        # List of default values
        self._default_values = { }
        # Optional file to read from
        self.__filen = filen
        if self.__filen is None:
            # No existing file so set the default
            # fields to write to the file
            self._fields = self._default_fields
        else:
            # Get columns from existing file
            with open(self.__filen,'r') as fp:
                header = fp.readline()
                self._fields = header.rstrip('\n').lstrip('#').split('\t')
        # Open the file
        TabFile.TabFile.__init__(self,filen=self.__filen,
                                 column_names=self._fields,
                                 first_line_is_header=True,
                                 convert=False)
        # Add any missing columns
        for field in self._default_fields:
            if field not in self._fields:
                self.appendColumn(field)

    def add_project(self,project_name,sample_names,**kws):
        """Add information about a project into the file

        Arguments:
          project_name: name of the project
          sample_names: Python list of sample names
          user: (optional) user name(s)
          library_type: (optional) library type
          organism: (optional) organism(s)
          PI: (optional) principal investigator name(s)
          comments: (optional) additional information about
            the project

        """
        # Check project name doesn't already exist
        if project_name in [p[self._fields[0]] for p in self]:
            raise Exception("Project '%s' already exists" %
                            project_name)
        # Assemble dictionary with all values
        values = { 'project_name': project_name,
                   'sample_names': ','.join(sample_names), }
        for kw in kws:
            values[kw] = kws[kw]
        # Build the data line to append to the file
        data = []
        for field in self._fields:
            # Identify the keyword parameter for this field
            try:
                kw = self._kwmap[field]
            except KeyError,ex:
                raise ex
            # Look up the assigned value
            try:
                value = values[kw]
            except KeyError:
                value = None
            # Append to the data line
            if value is None:
                data.append('.')
            else:
                data.append(value)
        # Add project info to the metadata file
        self.append(data=data)

    def project(self,name):
        """Return AttributeDictionary for a project

        """
        raise NotImplementedError

    def save(self,filen=None):
        """Save the data back to file

        Arguments:
          filen: name of the file to save to (if not specified then
            defaults to the same file as data was read in from)

        """
        if filen is not None:
            self.__filen = filen
        self.write(filen=self.__filen,include_header=True)

class OutputFiles:
    """Class for managing multiple output files

    Usage:

    Create a new OutputFiles instance:
    >>> fp = OutputFiles()

    Set up files against keys:
    >>> fp.open('file1','first_file.txt')
    >>> fp.open('file2','second_file.txt')

    Write content to files:
    >>> fp.write('file1','some content for first file')
    >>> fp.write('file2','content for\nsecond file')

    Append content to an existing file:
    >>> fp.open('file3','third_file.txt',append=True)
    >>> fp.write('file2','appended content')

    Check if key exists and associated file handle is
    available for writing:
    >>> 'file1' in fp
    True
    >>> 'file3' in fp
    False

    Finish and close all open files
    >>> fp.close()

    Reopen and append to a previously opened and closed
    file:
    >>> fp.open('file4','fourth_file.txt')
    >>> fp.write('file4','some content')
    >>> fp.close('file4')
    >>> fp.open('file4',append=True)
    >>> fp.write('file4','more content')

    """
    def __init__(self,base_dir=None):
        """Create a new OutputFiles instance

        Arguments:
          base_dir (str): optional 'base' directory
            which files will be created relative to

        """
        self._fp = dict()
        self._file = dict()
        self._base_dir = base_dir

    def open(self,name,filen=None,append=False):
        """Open a new output file

        'name' is the handle used to reference the
        file when using the 'write' and 'close' methods.

        'filen' is the name of the file, and is unrelated
        to the handle. If not supplied then 'name' must
        be associated with a previously closed file (which
        will be reopened).

        If 'append' is True then append to an existing
        file rather than overwriting (i.e. use mode 'a'
        instead of 'w').

        """
        if append:
            mode = 'a'
        else:
            mode = 'w'
        if filen is None:
            filen = self.file_name(name)
        elif self._base_dir is not None:
            filen = os.path.join(self._base_dir,filen)
        else:
            filen = os.path.abspath(filen)
        self._file[name] = filen
        self._fp[name] = open(filen,mode)

    def write(self,name,s):
        """Write content to file (newline-terminated)

        Writes 's' as a newline-terminated string to the
        file that is referenced with the handle 'name'.

        """
        self._fp[name].write("%s\n" % s)

    def file_name(self,name):
        """Get the file name associated with a handle

        NB the file name will be available even if the
        file has been closed.

        Raises KeyError if the key doesn't exist.

        """
        return self._file[name]

    def close(self,name=None):
        """Close one or all open files

        If a 'name' is specified then only the file matching
        that handle will be closed; with no arguments all
        open files will be closed.

        """
        if name is not None:
            self._fp[name].close()
            del(self._fp[name])
        else:
            names = self._fp.keys()
            for name in names:
                self.close(name)

    def __contains__(self,name):
        return name in self._fp

    def __len__(self):
        return len(self._fp.keys())

class ZipArchive(object):
    """
    Utility class for creating .zip archive files

    Example usage:

    >>> z = ZipArchive('test.zip',relpath='/data')
    >>> z.add('/data/file1') # Add a single file
    >>> z.add('/data/dir2/') # Add a directory and all contents
    >>> z.close()  # to write the archive

    """
    def __init__(self,zip_file,contents=None,relpath=None,prefix=None):
        """
        Make an new zip archive instance

        Arguments:
          zip_file (str): path to the zip file to be created
          contents (list): list of file and/or directory paths
            which will be added to the zip file
          relpath (str): optional, if specified then this path
            will be stripped from the leading path for each item
            before being written (see also 'prefix')
          prefix (str): optional, if specified then this path
            will be prepended to the names of the items written
            to the archive. The prepending takes place after the
            relpath argument has been applied

        """
        self._zipfile = zipfile.ZipFile(zip_file,'w',
                                        allowZip64=True)
        self._relpath = relpath
        self._prefix = prefix
        if contents is not None:
            for item in contents:
                self.add(item)

    def add(self,item):
        """
        Add an item (file or directory) to the zip archive
        """
        item = os.path.abspath(item)
        if os.path.isfile(item):
            # Add file
            self.add_file(item)
        elif os.path.isdir(item):
            # Add directory and contents
            self.add_dir(item)
        else:
            raise Exception("ZipArchive: unknown item type for '%s'"
                            % item)

    def add_file(self,filen):
        """
        Add a file to the zip archive
        """
        if self._relpath:
            zip_pth = os.path.relpath(filen,self._relpath)
        else:
            zip_pth = filen
        if self._prefix:
            zip_pth = os.path.join(self._prefix,zip_pth)
        self._zipfile.write(filen,zip_pth)

    def add_dir(self,dirn):
        """
        Recursively add a directory and its contents
        """
        for item in os.listdir(dirn):
            f = os.path.join(dirn,item)
            if os.path.isdir(f):
                self.add_dir(f)
            else:
                self.add_file(f)

    def close(self):
        self._zipfile.close()

    def __del__(self):
        self.close()

#######################################################################
# Functions
#######################################################################

def bases_mask_is_paired_end(bases_mask):
    # Determine if run is paired end based on bases mask string
    non_index_reads = []
    for read in bases_mask.split(','):
        try:
            read.index('I')
        except ValueError:
            non_index_reads.append(read)
    if len(non_index_reads) == 2:
        # Paired end
        return True
    elif len(non_index_reads) < 2:
        # Single end
        return False
    else:
        # An error?
        raise Exception, "Bad bases mask '%s'?" % bases_mask

def split_user_host_dir(location):
    # Split a location of the form [[user@]host:]dir into its
    # user, hostname and directory components
    try:
        location = location.strip()
    except AttributeError:
        # Not a string?
        logger.error("Bad input to split_user_host_dir: '%s'" % location)
        return (None,None,None)
    if not location:
        return (None,None,None)
    try:
        location.index(':')
        location,dirn = location.split(':')
        try:
            location.index('@')
            user,host = location.split('@')
        except ValueError:
            user = None
            host = location
    except ValueError:
        user = None
        host = None
        dirn = location
    return (user,host,dirn)

def pretty_print_rows(data,prepend=False):
    """Format row-wise data into 'pretty' lines

    Given 'row-wise' data (in the form of a list of lists),
    for example:

    [['hello','A salutation'],[goodbye','The End']]

    formats into a string of text where lines are
    newline-separated and the 'fields' are padded with
    spaces so that they line up left-justified in
    columns, for example:

    hello   A salutation
    goodbye The End

    Arguments:
      data: row-wise data as a list of lists
      prepend: (optional), if True then columns
        are right-justified (i.e. padding is
        added before each value).

    """
    # Get maximum field widths for each column
    widths = []
    for row in data:
        for i in range(len(row)):
            width = len(str(row[i]))
            try:
                widths[i] = max(width,widths[i])
            except IndexError:
                widths.append(width)
    # Build output
    output = []
    for row in data:
        line = []
        for item,width in zip([str(x) for x in row],widths):
            padding = ' '*(width-len(item))
            if prepend:
                line.append(padding + item)
            else:
                line.append(item + padding)
        output.append(' '.join(line))
    return '\n'.join(output)

def write_script_file(script_file,contents,append=False,shell=None):
    """Write command to file

    Arguments:
      script_file (str): path of file to write command to
      contents (str): content to write to the file
      append (bool): optional, if True and script_file exists
        then append content (default is to overwrite existing
        contents) 
      shell: optional, if set then defines the shell to
        specify after '!#'

    """
    if append:
        mode = 'a'
    else:
        mode = 'w'
    with open(script_file,mode=mode) as fp:
        if (not append) and (shell is not None):
            fp.write("#!%s\n" % shell)
        fp.write("%s\n" % contents)
    os.chmod(script_file,0775)

def edit_file(filen,editor="vi",append=None):
    """
    Send a file to an editor

    Creates a temporary copy of a file and opens an
    editor to allow the user to make changes. Any
    edits are saved back to the original file.

    Arguments:
      filen (str): path to the file to be edited
      editor (str): optional, editor command to be used
        (will be overriden by user's EDITOR environment
        variable even if set). Defaults to 'vi'.
      append (str): optional, if set then append the
        supplied text to the end of the file before
        editing. NB the text will only be kept if the
        user saves a change to the file in the editor.

    """
    # Acquire an editor command
    try:
        editor = os.environ["EDITOR"]
    except KeyError:
        pass
    if editor is None:
        logger.critical("No editor specified!")
        return
    # Make a temporary copy for editing
    f,tmpfile = tempfile.mkstemp()
    os.fdopen(f).close()
    with open(tmpfile,'w') as fp:
        if os.path.exists(filen):
            fp.write(open(filen,'r').read())
        else:
            fp.write()
        if append:
            fp.write("%s\n" % str(append))
    checksum = md5sum(tmpfile)
    # Build command line to run the editor
    editor = str(editor).split(' ')
    edit_cmd = applications.Command(editor[0],*editor[1:])
    edit_cmd.add_args(tmpfile)
    edit_cmd.run_subprocess()
    # Finished
    if md5sum(tmpfile) != checksum:
        with open(filen,'w') as fp:
            fp.write(open(tmpfile,'r').read())
            os.remove(tmpfile)
    else:
        logger.warning("no changes to write")

def paginate(text):
    """
    Send text to stdout with pagination

    If the function detects that the stdout is an interactive
    terminal then the supplied text will be piped via a
    paginator command.

    The pager command will be the default for ``pydoc``, but
    can be over-ridden by the ``PAGER`` environment variable.

    If stdout is not a terminal (for example if it's being
    set to a file, or piped to another command) then the
    pagination is skipped.

    Arguments:
      text (str): text to be printed using pagination

    """
    # If stdout is a terminal
    if os.isatty(sys.stdout.fileno()):
        # Acquire a pager command
        try:
            pager = os.environ["PAGER"]
        except KeyError:
            pager = None
        # Output the prediction with paging
        if pager is not None:
            pydoc.pipepager(text,cmd=pager)
        else:
            pydoc.pager(text)
    else:
        # Stdout not a terminal
        print text
